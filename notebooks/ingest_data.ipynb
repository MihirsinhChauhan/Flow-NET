{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Mihir Chauhan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, target_size=(100, 100)):\n",
    "    \"\"\"\n",
    "    Preprocess a single frame:\n",
    "    - Resize to target size\n",
    "    - Convert to grayscale\n",
    "    \"\"\"\n",
    "    # Resize frame\n",
    "    frame_resized = cv2.resize(frame, target_size)\n",
    "    # Convert to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "    return frame_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping from label names to numerical values\n",
    "label_map = {'HAS': 0, 'LAS': 1, 'Plug': 2, 'Stratified': 3}\n",
    "\n",
    "\n",
    "def load_frames_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load frames from a folder and preprocess them.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get list of image files in the folder\n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        try:\n",
    "            # Read image\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            frame = cv2.imread(image_path)\n",
    "            if frame is None:\n",
    "                continue\n",
    "            \n",
    "            # Preprocess frame\n",
    "            processed_frame = preprocess_frame(frame)\n",
    "            \n",
    "            # Extract label from folder name\n",
    "            label_name = os.path.basename(folder_path)\n",
    "            label = label_map.get(label_name)\n",
    "            if label is None:\n",
    "                continue\n",
    "            # Append processed frame and label to lists\n",
    "            frames.append(processed_frame)\n",
    "            labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image '{image_file}': {e}\")\n",
    "    \n",
    "    return frames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to load frames from three folders\n",
    "def load_frames_main(data_folders):\n",
    "    all_frames = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for folder in data_folders:\n",
    "        frames, labels = load_frames_from_folder(folder)\n",
    "        all_frames.extend(frames)\n",
    "        all_labels.extend(labels)\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_frames = np.array(all_frames)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    return all_frames, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frames array: (20163, 100, 100)\n",
      "Unique labels: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# List of folders containing frames, each representing a different class\n",
    "\n",
    "data_folder_HAS = '../../Two Phase Data/HAS'\n",
    "data_folder_LAS = '../../Two Phase Data/LAS'\n",
    "data_folder_Plug = '../../Two Phase Data/Plug'\n",
    "data_folder_Stratified = '../../Two Phase Data/Stratified'\n",
    "\n",
    "data_folders = [data_folder_HAS, data_folder_LAS, data_folder_Plug, data_folder_Stratified]\n",
    "\n",
    "# Load frames and labels from folders\n",
    "frames, labels = load_frames_main(data_folders)\n",
    "\n",
    "# Check the shape of the loaded frames array\n",
    "print(\"Shape of frames array:\", frames.shape)\n",
    "\n",
    "# Check the unique labels\n",
    "print(\"Unique labels:\", np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 12904\n",
      "Number of samples in validation set: 3226\n",
      "Number of samples in test set: 4033\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(frames, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert numpy arrays to TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "# Optionally shuffle and batch the datasets\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# Print the number of samples in each dataset\n",
    "print(\"Number of samples in training set:\", len(X_train))\n",
    "print(\"Number of samples in validation set:\", len(X_val))\n",
    "print(\"Number of samples in test set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define file paths to save the preprocessed data\n",
    "train_data_path = '../data/train_data.npz'\n",
    "val_data_path = '../data/val_data.npz'\n",
    "test_data_path = '../data/test_data.npz'\n",
    "\n",
    "# Save the preprocessed data as NumPy arrays\n",
    "np.savez(train_data_path, frames=X_train, labels=y_train)\n",
    "np.savez(val_data_path, frames=X_val, labels=y_val)\n",
    "np.savez(test_data_path, frames=X_test, labels=y_test)\n",
    "\n",
    "print(\"Preprocessed data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding manual bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def create_biased_data(image_folder, num_swaps):\n",
    "    \"\"\"\n",
    "    Create bias in image data by swapping images from different classes.\n",
    "    Args:\n",
    "        image_folder (str): Path to the folder containing image data.\n",
    "        num_swaps (int): Number of swaps to perform.\n",
    "    \"\"\"\n",
    "    # Get list of class folders\n",
    "    class_folders = [f for f in os.listdir(image_folder) if os.path.isdir(os.path.join(image_folder, f))]\n",
    "    print(class_folders)\n",
    "    for _ in range(num_swaps):\n",
    "        # Randomly select two different classes\n",
    "        class1, class2 = random.sample(class_folders, 2)\n",
    "        \n",
    "        # Get list of images in each class\n",
    "        class1_images = [f for f in os.listdir(os.path.join(image_folder, class1)) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        class2_images = [f for f in os.listdir(os.path.join(image_folder, class2)) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "        \n",
    "        # Randomly select an image from each class\n",
    "        if class1_images and class2_images:\n",
    "            image1 = random.choice(class1_images)\n",
    "            image2 = random.choice(class2_images)\n",
    "            \n",
    "            # Swap images\n",
    "            shutil.move(os.path.join(image_folder, class1, image1), os.path.join(image_folder, class2, image1))\n",
    "            shutil.move(os.path.join(image_folder, class2, image2), os.path.join(image_folder, class1, image2))\n",
    "        else:\n",
    "            print(\"Not enough images in one or both classes.\")\n",
    "    \n",
    "    print(f\"Created bias by swapping images from different classes for {num_swaps} swaps.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HAS', 'LAS', 'Plug', 'Stratified']\n",
      "Created bias by swapping images from different classes for 150 swaps.\n"
     ]
    }
   ],
   "source": [
    "image_folder = '../../Two Phase Data'\n",
    "num_swaps = 150  # Number of swaps to perform\n",
    "create_biased_data(image_folder, num_swaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frames array: (18923, 100, 100)\n",
      "Unique labels: [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# List of folders containing frames, each representing a different class\n",
    "\n",
    "data_folder_HAS = '../../Two Phase Data/HAS'\n",
    "data_folder_LAS = '../../Two Phase Data/LAS'\n",
    "data_folder_Plug = '../../Two Phase Data/Plug'\n",
    "data_folder_Stratified = '../../Two Phase Data/Stratified'\n",
    "\n",
    "data_folders = [data_folder_HAS, data_folder_LAS, data_folder_Plug, data_folder_Stratified]\n",
    "\n",
    "# Load frames and labels from folders\n",
    "frames, labels = load_frames_main(data_folders)\n",
    "\n",
    "# Check the shape of the loaded frames array\n",
    "print(\"Shape of frames array:\", frames.shape)\n",
    "\n",
    "# Check the unique labels\n",
    "print(\"Unique labels:\", np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set: 12110\n",
      "Number of samples in validation set: 3028\n",
      "Number of samples in test set: 3785\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(frames, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert numpy arrays to TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "# Optionally shuffle and batch the datasets\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# Print the number of samples in each dataset\n",
    "print(\"Number of samples in training set:\", len(X_train))\n",
    "print(\"Number of samples in validation set:\", len(X_val))\n",
    "print(\"Number of samples in test set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define file paths to save the preprocessed data\n",
    "train_data_path = '../data/bias_train_data.npz'\n",
    "val_data_path = '../data/bias_val_data.npz'\n",
    "test_data_path = '../data/bias_test_data.npz'\n",
    "\n",
    "# Save the preprocessed data as NumPy arrays\n",
    "np.savez(train_data_path, frames=X_train, labels=y_train)\n",
    "np.savez(val_data_path, frames=X_val, labels=y_val)\n",
    "np.savez(test_data_path, frames=X_test, labels=y_test)\n",
    "\n",
    "print(\"Preprocessed data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
